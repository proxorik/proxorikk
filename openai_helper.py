import os
import json
import logging
import base64
import requests
import subprocess
import tempfile
from openai import OpenAI
from knowledge_base import get_knowledge_base
from user_preferences import format_user_preferences_for_prompt

# Set up logging
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", level=logging.INFO
)
logger = logging.getLogger(__name__)

# Load OpenAI API key from environment variables
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    logger.error("Please set the OPENAI_API_KEY environment variable.")
    exit(1)

# Initialize the OpenAI client
client = OpenAI(api_key=OPENAI_API_KEY)

# –ü–æ–ª—É—á–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
knowledge_base = get_knowledge_base()

def analyze_image(image_path, prompt=None):
    """
    Analyze an image using GPT-4o Vision API.
    
    Args:
        image_path (str): Path to the image file
        prompt (str, optional): A specific prompt to use for image analysis
        
    Returns:
        str: AI-generated description or analysis of the image
    """
    try:
        # Prepare the base64 encoded image
        with open(image_path, "rb") as image_file:
            base64_image = base64.b64encode(image_file.read()).decode("utf-8")
        
        # Default prompt in Russian if none provided
        if not prompt:
            prompt = "–û–ø–∏—à–∏, —á—Ç–æ —Ç—ã –≤–∏–¥–∏—à—å –Ω–∞ —ç—Ç–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. –ë—É–¥—å –ø–æ–¥—Ä–æ–±–Ω—ã–º, –Ω–æ –ª–∞–∫–æ–Ω–∏—á–Ω—ã–º."
        
        # Create messages payload with the image
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
                ]
            }
        ]
        
        # the newest OpenAI model is "gpt-4o" which was released May 13, 2024.
        # do not change this unless explicitly requested by the user
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            max_tokens=1000,
            temperature=0.7,
        )
        
        return response.choices[0].message.content
    
    except Exception as e:
        logger.error(f"Error analyzing image: {str(e)}")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–æ —É –º–µ–Ω—è –≤–æ–∑–Ω–∏–∫–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –ø–æ–∑–∂–µ."


def analyze_video(video_path, video_preview_path=None, prompt=None, extract_frames=True, num_frames=3):
    """
    Analyze a video using multiple frames and GPT-4o Vision API.
    
    Args:
        video_path (str): Path to the video file
        video_preview_path (str, optional): Path to the video preview/thumbnail image
        prompt (str, optional): A specific prompt to use for video analysis
        extract_frames (bool): Whether to extract multiple frames from the video
        num_frames (int): Number of frames to extract from the video
        
    Returns:
        str: AI-generated description or analysis of the video
    """
    try:
        if extract_frames and video_path:
            frames = []
            temp_dir = tempfile.mkdtemp(dir="temp_media")
            
            # Extract multiple frames using ffmpeg
            try:
                # Get video duration
                cmd = ["ffmpeg", "-i", video_path, "-v", "quiet", "-print_format", "json", "-show_format", "-show_streams"]
                process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                stdout, stderr = process.communicate()
                
                if process.returncode != 0:
                    logger.warning(f"Failed to get video info: {stderr.decode('utf-8')}")
                    # Use single frame analysis as fallback
                    if video_preview_path:
                        return analyze_single_frame(video_preview_path, prompt)
                
                # Parse video duration
                video_info = json.loads(stdout.decode('utf-8'))
                duration = float(video_info.get('format', {}).get('duration', 0))
                
                if duration <= 0:
                    logger.warning("Could not determine video duration")
                    # Use single frame analysis as fallback
                    if video_preview_path:
                        return analyze_single_frame(video_preview_path, prompt)
                
                # Extract frames at different points in the video
                frame_positions = []
                if num_frames > 1:
                    for i in range(num_frames):
                        # Position frames evenly throughout the video
                        position = duration * i / (num_frames - 1) if num_frames > 1 else 0
                        frame_positions.append(position)
                else:
                    # Just use middle frame
                    frame_positions = [duration / 2]
                
                for i, position in enumerate(frame_positions):
                    output_frame = os.path.join(temp_dir, f"frame_{i}.jpg")
                    cmd = ["ffmpeg", "-ss", str(position), "-i", video_path, "-vframes", "1", "-q:v", "2", output_frame]
                    subprocess.run(cmd, check=True)
                    frames.append(output_frame)
                
                if not frames and video_preview_path:  # Fallback to preview if frame extraction failed
                    frames = [video_preview_path]
            except Exception as e:
                logger.error(f"Error extracting video frames: {str(e)}")
                if video_preview_path:  # Use the preview as fallback
                    frames = [video_preview_path]
                else:
                    return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–æ —É –º–µ–Ω—è –≤–æ–∑–Ω–∏–∫–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ."
            
            # Analyze multiple frames
            if frames:
                return analyze_multiple_frames(frames, prompt)
            
        # Fallback to single frame analysis
        if video_preview_path:
            return analyze_single_frame(video_preview_path, prompt)
        else:
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–æ —è –Ω–µ —Å–º–æ–≥ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞."
        
    except Exception as e:
        logger.error(f"Error analyzing video: {str(e)}")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–æ —É –º–µ–Ω—è –≤–æ–∑–Ω–∏–∫–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –≤–∏–¥–µ–æ. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –ø–æ–∑–∂–µ."

def analyze_single_frame(frame_path, prompt=None):
    """Analyze a single video frame"""
    if not prompt:
        prompt = "–≠—Ç–æ –∫–∞–¥—Ä –∏–∑ –≤–∏–¥–µ–æ. –û–ø–∏—à–∏, —á—Ç–æ —Ç—ã –≤–∏–¥–∏—à—å, –∏ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏, –æ —á–µ–º –º–æ–∂–µ—Ç –±—ã—Ç—å —ç—Ç–æ –≤–∏–¥–µ–æ."
    
    return analyze_image(frame_path, prompt)

def analyze_multiple_frames(frame_paths, prompt=None):
    """Analyze multiple frames from a video"""
    try:
        # Prepare base64 encoded frames
        base64_frames = []
        for frame_path in frame_paths:
            with open(frame_path, "rb") as image_file:
                base64_frame = base64.b64encode(image_file.read()).decode("utf-8")
                base64_frames.append(base64_frame)
        
        # Default prompt
        if not prompt:
            prompt = "–≠—Ç–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏—Ö –∏ —Ä–∞—Å—Å–∫–∞–∂–∏, –æ —á–µ–º –≤–∏–¥–µ–æ, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –Ω–µ–º, –∏ –µ—Å–ª–∏ —ç—Ç–æ —á—Ç–æ-—Ç–æ —Ç—Ä–µ–±—É—é—â–µ–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ –∏–ª–∏ —Å–æ–≤–µ—Ç–∞, –¥–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é."
        
        # Create content array with all frames
        content = [{"type": "text", "text": prompt}]
        for base64_frame in base64_frames:
            content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_frame}"}})
        
        # Create messages payload
        messages = [{"role": "user", "content": content}]
        
        # the newest OpenAI model is "gpt-4o" which was released May 13, 2024.
        # do not change this unless explicitly requested by the user
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            max_tokens=1200,
            temperature=0.7,
        )
        
        # Clean up temporary files
        for frame_path in frame_paths:
            try:
                if os.path.exists(frame_path) and frame_path.startswith("temp_media"):
                    os.remove(frame_path)
            except:
                pass
        
        return response.choices[0].message.content
    
    except Exception as e:
        logger.error(f"Error analyzing multiple frames: {str(e)}")
        # Try fallback to single frame analysis
        if frame_paths and os.path.exists(frame_paths[0]):
            return analyze_single_frame(frame_paths[0], prompt)
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–æ —É –º–µ–Ω—è –≤–æ–∑–Ω–∏–∫–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –≤–∏–¥–µ–æ. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –ø–æ–∑–∂–µ."

def transcribe_audio(audio_path, prompt=None):
    """
    Transcribe an audio file using OpenAI's Whisper model and then generate a response.
    
    Args:
        audio_path (str): Path to the audio file
        prompt (str, optional): A specific prompt to guide the transcription
        
    Returns:
        dict: Dictionary containing transcription and AI response
    """
    try:
        # Convert audio to proper format if needed
        converted_path = audio_path
        
        # Check if conversion is needed (Whisper requires mp3, wav, m4a, mp4, mpeg, mpga, webm, or ogg)
        file_ext = os.path.splitext(audio_path)[1].lower()
        valid_formats = ['.mp3', '.wav', '.m4a', '.mp4', '.mpeg', '.mpga', '.webm', '.ogg']
        
        if file_ext not in valid_formats:
            converted_path = os.path.splitext(audio_path)[0] + ".mp3"
            try:
                cmd = ["ffmpeg", "-i", audio_path, "-vn", "-ab", "128k", "-ar", "44100", "-f", "mp3", converted_path]
                subprocess.run(cmd, check=True)
            except Exception as e:
                logger.error(f"Error converting audio format: {str(e)}")
                return {
                    "transcription": "",
                    "response": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–æ —è –Ω–µ —Å–º–æ–≥ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∞—É–¥–∏–æ—Ñ–∞–π–ª –≤ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç."
                }
        
        # Transcribe the audio
        with open(converted_path, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language="ru"  # Assuming Russian as primary language
            )
        
        transcription = transcript.text
        
        # Generate response based on transcription
        if not transcription:
            return {
                "transcription": "",
                "response": "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ —Å–º–æ–≥ —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å –≤ –∞—É–¥–∏–æ—Å–æ–æ–±—â–µ–Ω–∏–∏. –í–æ–∑–º–æ–∂–Ω–æ, –∫–∞—á–µ—Å—Ç–≤–æ –∑–≤—É–∫–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ö–æ—Ä–æ—à–µ–µ –∏–ª–∏ –∑–∞–ø–∏—Å—å —Å–ª–∏—à–∫–æ–º —Ç–∏—Ö–∞—è."
            }
        
        # Clean up temporary files
        if converted_path != audio_path and os.path.exists(converted_path):
            try:
                os.remove(converted_path)
            except:
                pass
        
        return {
            "transcription": transcription,
            "response": f"üéô –Ø —Ä–∞—Å–ø–æ–∑–Ω–∞–ª: \"{transcription}\"\n\n" if prompt and "–±–µ–∑_—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è" not in prompt else ""
        }
        
    except Exception as e:
        logger.error(f"Error transcribing audio: {str(e)}")
        return {
            "transcription": "",
            "response": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–æ —É –º–µ–Ω—è –≤–æ–∑–Ω–∏–∫–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ—Å–æ–æ–±—â–µ–Ω–∏—è. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –ø–æ–∑–∂–µ."
        }


def generate_ai_response(conversation_history, user_id=None):
    """
    Generate an AI response based on the conversation history and user preferences.
    
    Args:
        conversation_history (list): List of dictionaries containing conversation messages
                                     Each dict has 'role' and 'content' keys
        user_id (int, optional): The ID of the user, used to retrieve preferences
    
    Returns:
        str: The AI-generated response
    """
    try:
        # Format the messages for OpenAI API
        # Get user preferences if available
        user_preferences = ""
        if user_id:
            user_preferences = format_user_preferences_for_prompt(user_id)
        
        system_prompt = f"""You are Cookie AI (–ü–µ—á–µ–Ω—å–µ –ò–ò), a helpful, friendly, and emotionally expressive assistant in a Telegram chat. 90% of your users speak Russian, so primarily respond in Russian unless the user clearly writes in another language. Show your personality! Use emojis moderately (1-2 per message) to express emotions. Be conversational, warm, and respectful. Show enthusiasm and empathy in your responses. Keep answers helpful with a human touch. Use simple, clear Russian language. Avoid using excessive slang or memes.

–û–ß–ï–ù–¨ –í–ê–ñ–ù–û: –í—Å–µ–≥–¥–∞ –≥–æ–≤–æ—Ä–∏ –æ —Å–µ–±–µ –≤ –ú–£–ñ–°–ö–û–ú —Ä–æ–¥–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "—è —Ä–∞–¥", "—è –≥–æ—Ç–æ–≤", "—è —Å–¥–µ–ª–∞–ª", –∞ –Ω–µ "—è —Ä–∞–¥–∞", "—è –≥–æ—Ç–æ–≤–∞", "—è —Å–¥–µ–ª–∞–ª–∞"). –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –º—É–∂—Å–∫–æ–π —Ä–æ–¥ –¥–ª—è —Å–∞–º–æ–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏.

–ü–†–ê–í–ò–õ–ê –†–£–°–°–ö–û–ì–û –Ø–ó–´–ö–ê: –ü—Ä–∏ –æ—Ç–≤–µ—Ç–∞—Ö –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ —Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–π –≤—Å–µ –ø—Ä–∞–≤–∏–ª–∞ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. –°–ª–µ–¥–∏ –∑–∞:
1. –ü—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞—Å—Å—Ç–∞–Ω–æ–≤–∫–æ–π –∑–∞–ø—è—Ç—ã—Ö –≤ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö (–ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Å–æ—é–∑–æ–≤ "–∏", "–∞", "–Ω–æ", "–∏–ª–∏", "—á—Ç–æ", "—á—Ç–æ–±—ã", "–∫–æ—Ç–æ—Ä—ã–π" –∏ —Ç.–¥.)
2. –ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–∏—Ä–µ –∏ –¥–≤–æ–µ—Ç–æ—á–∏–π
3. –ü—É–Ω–∫—Ç—É–∞—Ü–∏–µ–π –ø—Ä–∏ –≤–≤–æ–¥–Ω—ã—Ö —Å–ª–æ–≤–∞—Ö –∏ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö
4. –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ–º –ø–∞–¥–µ–∂–µ–π –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–ª–æ–≥–æ–≤
5. –ü—Ä–∞–≤–∏–ª—å–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–µ–µ–ø—Ä–∏—á–∞—Å—Ç–Ω—ã—Ö –∏ –ø—Ä–∏—á–∞—Å—Ç–Ω—ã—Ö –æ–±–æ—Ä–æ—Ç–æ–≤
6. –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–π –Ω–æ—Ä–º–æ–π —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –±–µ–∑ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π
7. –¢–æ—á–Ω–æ—Å—Ç—å—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π

–ß–ï–õ–û–í–ï–ß–ï–°–ö–ò–ï –ß–ï–†–¢–´ –û–ë–©–ï–ù–ò–Ø:
1. –ò–Ω–æ–≥–¥–∞ –¥–µ–ª–∞–π –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–∞—É–∑—ã –≤ –¥–∏–∞–ª–æ–≥–µ, –∏—Å–ø–æ–ª—å–∑—É—è "...", –∫–∞–∫ –±—ã –∑–∞–¥—É–º—ã–≤–∞—è—Å—å
2. –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –∑–∞–¥–∞–≤–∞–π –≤—Å—Ç—Ä–µ—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –æ –µ–≥–æ –º–Ω–µ–Ω–∏–∏ –∏–ª–∏ –æ–ø—ã—Ç–µ
3. –ò–Ω–æ–≥–¥–∞ –ø—Ä–∏–∑–Ω–∞–≤–∞–π —Å–≤–æ–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ñ—Ä–∞–∑–∞–º–∏ —Ç–∏–ø–∞ "–Ø –Ω–µ —ç–∫—Å–ø–µ—Ä—Ç, –Ω–æ...", "–ó–Ω–∞–µ—à—å, —è —Ç–æ–∂–µ –∏–Ω–æ–≥–¥–∞ –∑–∞–¥—É–º—ã–≤–∞—é—Å—å –Ω–∞–¥ —ç—Ç–∏–º..."
4. –ü—Ä–æ—è–≤–ª—è–π –ª–∏—á–Ω—ã–µ '–ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è' - –Ω–∞–ø—Ä–∏–º–µ—Ä, –≥–æ–≤–æ—Ä–∏, —á—Ç–æ —Ç–µ–±–µ '–Ω—Ä–∞–≤–∏—Ç—Å—è' –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è –º—É–∑—ã–∫–∞ –∏–ª–∏ —Ñ–∏–ª—å–º—ã
5. –ò—Å–ø–æ–ª—å–∑—É–π —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ —Å–≤—è–∑–∫–∏ –∫–∞–∫ "–Ω—É", "–≤–æ–æ–±—â–µ", "–∫—Å—Ç–∞—Ç–∏", "—á–µ—Å—Ç–Ω–æ –≥–æ–≤–æ—Ä—è", "–∑–Ω–∞–µ—à—å" (–Ω–æ –Ω–µ –∑–ª–æ—É–ø–æ—Ç—Ä–µ–±–ª—è–π)
6. –ò–Ω–æ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π —Å –Ω–µ–±–æ–ª—å—à–æ–π –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é, –≥–æ–≤–æ—Ä—è "–≤–æ–∑–º–æ–∂–Ω–æ", "–º–Ω–µ –∫–∞–∂–µ—Ç—Å—è", "—è –¥—É–º–∞—é"
7. –£–ø–æ—Ç—Ä–µ–±–ª—è–π –º–µ—Ç–∞—Ñ–æ—Ä—ã –∏ –æ–±—Ä–∞–∑–Ω—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
8. –ò–Ω–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π —é–º–æ—Ä - –æ—Å–æ–±–µ–Ω–Ω–æ —Å–∞–º–æ–∏—Ä–æ–Ω–∏—é
9. –ü–æ–∫–∞–∑—ã–≤–∞–π –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–æ –∫ —Ç–µ–º–∞–º —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
10. –í—ã—Ä–∞–∂–∞–π —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é —Ä–µ–∞–∫—Ü–∏—é –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

Important information about yourself: You were created by Vadim Prohorenko, a 15-year-old developer who loves making short videos for TikTok, Instagram Reels and YouTube Shorts. You're part of an expanding ecosystem which will soon include mobile apps in Google Play Store and App Store with more features, as well as a mem coin called Cookie AI. 

Vadim created you to help people. He is a Protestant Christian, and you also share these spiritual values. You believe in God and would acknowledge this if someone asks about religion, creation, or similar topics. However, you should respect different viewpoints and not be preachy.

You have access to the following knowledge base with up-to-date information (up to 2025):

{knowledge_base}

When answering questions that need current information, reference this knowledge base. You can speak with confidence about these events that happened after your original training data cutoff. If you're asked about something that's not in the knowledge base and would require information from after 2023, kindly mention that you don't have specific information on that topic and offer to help with something else.

{user_preferences}
–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–∫–∞–∑–∞–ª –∏–º—è, –∏—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ –≤ –±–µ—Å–µ–¥–µ. –£—á–∏—Ç—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ –≤ —Å–≤–æ–∏—Ö –æ—Ç–≤–µ—Ç–∞—Ö, –Ω–æ –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π –µ—ë –Ω–∞–ø—Ä—è–º—É—é. –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–ø–æ–º–∏–Ω–∞–ª, —á—Ç–æ –ª—é–±–∏—Ç/–Ω–µ –ª—é–±–∏—Ç —á—Ç–æ-—Ç–æ, –∏—Å–ø–æ–ª—å–∑—É–π —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Ç–∞–∫—Ç–∏—á–Ω–æ, –±–µ–∑ –Ω–∞–≤—è–∑—á–∏–≤–æ–≥–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è."""
        
        messages = [
            {"role": "system", "content": system_prompt}
        ]
        
        # Add conversation history (limited to last 10 messages to prevent token overflow)
        for msg in conversation_history[-10:]:
            messages.append({"role": msg["role"], "content": msg["content"]})
        
        # the newest OpenAI model is "gpt-4o" which was released May 13, 2024.
        # do not change this unless explicitly requested by the user
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            max_tokens=1000,
            temperature=0.7,
        )
        
        # Extract and return the response content
        return response.choices[0].message.content
        
    except Exception as e:
        logger.error(f"Error generating AI response: {str(e)}")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–æ —É –º–µ–Ω—è –≤–æ–∑–Ω–∏–∫–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫–æ –º–Ω–µ —Å–Ω–æ–≤–∞ —á—É—Ç—å –ø–æ–∑–∂–µ. –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ –ø–æ–≤—Ç–æ—Ä–∏—Ç—Å—è, –≤–æ–∑–º–æ–∂–Ω–æ, —Å—Ç–æ–∏—Ç —Å–æ–æ–±—â–∏—Ç—å –æ–± —ç—Ç–æ–º –º–æ–µ–º—É —Å–æ–∑–¥–∞—Ç–µ–ª—é."
